
import { GoogleGenAI, Modality, GenerateContentResponse } from "@google/genai";

let generalAI: GoogleGenAI | null = null;
let liveAI: GoogleGenAI | null = null;

export const setApiKey = (apiKey: string) => {
  if (!apiKey) return;
  try {
    generalAI = new GoogleGenAI({ apiKey }); // Uses default v1beta
    liveAI = new GoogleGenAI({
      apiKey,
      httpOptions: { apiVersion: 'v1alpha' }
    });
  } catch (e) {
    console.error("Failed to set API Key:", e);
  }
};

// Initialize Logic: Prioritize LocalStorage (User entered), then Env (Build time)
const storedKey = typeof localStorage !== 'undefined' ? localStorage.getItem('gemini_api_key') : null;

if (storedKey) {
  setApiKey(storedKey);
} else if (process.env.API_KEY) {
  setApiKey(process.env.API_KEY);
}

// Models are now dynamic, but we keep this as a fallback/reference or for Flash.
// Pro model will be passed dynamically.
const transcriptionModelName = 'gemini-1.5-flash';
const NATIVE_AUDIO_MODEL = 'gemini-2.5-flash-native-audio-dialog-preview';

// 媛먯젙 ?꾨떖怨?臾몃㎘ ?좎?瑜??꾪빐 ???댁뿉 泥섎━???蹂?以???export const LINES_PER_TURN = 4;

interface SpeechConfig {
  voiceConfig?: {
    prebuiltVoiceConfig: {
      voiceName: string;
    };
  };
  multiSpeakerVoiceConfig?: {
    speakerVoiceConfigs: {
      speaker: string;
      voiceConfig: {
        prebuiltVoiceConfig: {
          voiceName: string;
        };
      };
    }[];
  };
  languageCode?: string;
}

/**
 * ?꾩뿭 ?먮윭 ?몃뱾?? API ?먮윭 ?묐떟??遺꾩꽍?섏뿬 ?ъ슜??移쒗솕?곸씤 硫붿떆吏濡?蹂?섑빀?덈떎.
 */
function handleApiError(error: any): Error {
  const message = error instanceof Error ? error.message : String(error);

  // 429 Too Many Requests ?먮뒗 Quota 愿???ㅼ썙??泥댄겕
  if (message.includes('429') || message.toLowerCase().includes('quota') || message.toLowerCase().includes('limit')) {
    return new Error("API ?붿껌 ?쒕룄(Quota)瑜?珥덇낵?덉뒿?덈떎. ?좊즺 怨꾩젙?대씪??紐⑤뜽蹂??쇱씪 ?앹꽦?됱씠??遺꾨떦 ?붿껌 ?쒗븳???덉쓣 ???덉뒿?덈떎. Google AI ?ㅽ뒠?붿삤??'Plan & Billing'?먯꽌 ?좊떦?됱쓣 ?뺤씤?섏떆嫄곕굹, ?좎떆(1~5遺? ???ㅼ떆 ?쒕룄??二쇱꽭??");
  }

  // 401/403 愿??(?몄쬆 ?먮윭)
  if (message.includes('401') || message.includes('403')) {
    return new Error("API ?ㅺ? ?좏슚?섏? ?딄굅??沅뚰븳???놁뒿?덈떎. ?ㅼ젙???뺤씤?댁＜?몄슂.");
  }

  // 500 愿??(?쒕쾭 ?먮윭)
  if (message.includes('500') || message.includes('503')) {
    return new Error("Google ?쒕쾭???쇱떆?곸씤 臾몄젣媛 諛쒖깮?덉뒿?덈떎. ?좎떆 ???ㅼ떆 ?쒕룄?댁＜?몄슂.");
  }

  return new Error(`AI? ?듭떊 以??ㅻ쪟媛 諛쒖깮?덉뒿?덈떎: ${message}`);
}

/**
 * Uint8Array瑜?硫붾え由??⑥쑉?곸쑝濡?Base64濡?蹂?섑빀?덈떎. (Stack Overflow 諛⑹?)
 */
export function uint8ArrayToBase64(uint8: Uint8Array): string {
  let binary = '';
  const chunkSize = 8192; // ?덉쟾??chunk ?ш린
  for (let i = 0; i < uint8.length; i += chunkSize) {
    const sub = uint8.subarray(i, i + chunkSize);
    binary += String.fromCharCode.apply(null, sub as any);
  }
  return btoa(binary);
}

/**
 * Base64 string to ArrayBuffer.
 */
function base64ToArrayBuffer(base64: string): ArrayBuffer {
  const binaryString = atob(base64);
  const bytes = new Uint8Array(binaryString.length);
  for (let i = 0; i < binaryString.length; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes.buffer;
}

/**
 * Merge multiple ArrayBuffers.
 */
function mergeArrayBuffers(buffers: ArrayBuffer[]): ArrayBuffer {
  const totalLength = buffers.reduce((acc, buf) => acc + buf.byteLength, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const buf of buffers) {
    result.set(new Uint8Array(buf), offset);
    offset += buf.byteLength;
  }
  return result.buffer;
}

/**
 * 吏?뺣맂 湲몄씠??臾댁쓬 PCM 踰꾪띁 ?앹꽦 (24kHz, 16bit, mono)
 */
function createSilenceBuffer(durationMs: number): ArrayBuffer {
  const sampleRate = 24000;
  const bytesPerSample = 2; // 16bit
  const numSamples = Math.floor((durationMs / 1000) * sampleRate);
  const buffer = new ArrayBuffer(numSamples * bytesPerSample);
  // ArrayBuffer??湲곕낯?곸쑝濡?0?쇰줈 珥덇린?붾맖 = 臾댁쓬
  return buffer;
}

/**
 * ?ㅻ뵒??泥?겕?ㅼ쓣 臾댁쓬 媛꾧꺽怨??④퍡 蹂묓빀
 */
function mergeAudioWithSilence(
  audioChunks: ArrayBuffer[],
  silenceMs: number = 500
): ArrayBuffer {
  const silence = createSilenceBuffer(silenceMs);
  const allBuffers: ArrayBuffer[] = [];

  for (let i = 0; i < audioChunks.length; i++) {
    allBuffers.push(audioChunks[i]);

    // 留덉?留?以꾩씠 ?꾨땲硫?臾댁쓬 異붽?
    if (i < audioChunks.length - 1) {
      allBuffers.push(silence);
    }
  }

  // ?꾩껜 蹂묓빀
  const totalLength = allBuffers.reduce((acc, buf) => acc + buf.byteLength, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const buf of allBuffers) {
    result.set(new Uint8Array(buf), offset);
    offset += buf.byteLength;
  }
  return result.buffer;
}

/**
 * 硫?고꽩 寃곌낵??lineTimings瑜??ъ슜??SRT ?앹꽦
 * 媛?臾몃떒(Batch)蹂꾨줈 ?섎굹??SRT 釉붾줉???앹꽦?⑸땲??
 */
export function generateSrtFromParagraphTimings(
  paragraphs: string[],
  lineTimings: Array<{ start: number; end: number }>
): string {
  const srtBlocks: string[] = [];

  for (let i = 0; i < paragraphs.length; i++) {
    const text = paragraphs[i].trim();
    if (!text) continue;

    const timing = lineTimings[i];
    if (!timing) continue;

    const startTime = msToSrtTime(timing.start);
    const endTime = msToSrtTime(timing.end);

    srtBlocks.push(`${srtBlocks.length + 1}\n${startTime} --> ${endTime}\n${text}\n`);
  }

  return srtBlocks.join('\n');
}

export function msToSrtTime(ms: number): string {
  const hours = Math.floor(ms / 3600000);
  const minutes = Math.floor((ms % 3600000) / 60000);
  const seconds = Math.floor((ms % 60000) / 1000);
  const milliseconds = Math.floor(ms % 1000);

  return `${String(hours).padStart(2, '0')}:${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')},${String(milliseconds).padStart(3, '0')}`;
}

async function _generateAudio(
  prompt: string,
  modelName: string,
  speechConfig: SpeechConfig,
  speed: number,
  stylePrompt?: string,
  signal?: AbortSignal
): Promise<string> {
  if (!generalAI || !liveAI) {
    throw new Error("API ?ㅺ? ?ㅼ젙?섏? ?딆븯?듬땲?? ?곗륫 ?곷떒 ?ㅼ젙 ?꾩씠肄섏쓣 ?뚮윭 API ?ㅻ? ?낅젰?댁＜?몄슂.");
  }

  // Pre-process text to avoid premature termination by ellipses in Live API
  const processedPrompt = prompt
    .replace(/\.\.\./g, ', ')
    .replace(/??g, ', ');

  try {
    const isNativeAudio = modelName.includes('native-audio-dialog');

    const config: {
      responseModalities: Modality[];
      speechConfig?: SpeechConfig;
    } = {
      responseModalities: [Modality.AUDIO],
      // Native Audio model doesn't use the standard speechConfig object in some versions, 
      // but we pass it as a hint.
      speechConfig: isNativeAudio ? undefined : speechConfig,
    };

    // Construct the prompt with instructions for steerability
    let finalPrompt = prompt;
    const instructions: string[] = [];

    // 1. Add Style Instructions if present
    if (stylePrompt && stylePrompt.trim().length > 0) {
      instructions.push(`Style/Tone: ${stylePrompt.trim()}`);
    }

    // 2. Add Speed Instructions if not normal
    if (speed !== 1.0) {
      instructions.push(`Speed: ${speed}x`);
    }

    // 3. For Standard TTS models, we add a persona hint to the instructions
    if (!isNativeAudio) {
      const voiceInfo = speechConfig.voiceConfig?.prebuiltVoiceConfig.voiceName;
      const isFemale = voiceInfo && ['Zephyr', 'Kore', 'Leda', 'Aoede', 'Callirrhoe', 'Autonoe', 'Despina', 'Erinome', 'Laomedeia', 'Gacrux', 'Pulcherrima', 'Vindemiatrix', 'Sulafat'].includes(voiceInfo);

      instructions.push(`Voice Category: ${isFemale ? 'FEMALE' : 'MALE'}`);
      // The user's stylePrompt is handled as the primary performance guide.
      // We only add the "Fixed Persona" as a subtle background baseline.
      instructions.push(`System Persona: Professional narrator with a calm and steady texture.`);
    }

    // 3. Combine Instructions and Script
    const contextPrefix = `[???띿뒪?몃뒗 ?꾪넻 ?쒓뎅 ?쇰떞/誘쇰떞?낅땲?? 援먯쑁 紐⑹쟻????궗??臾명븰 ?묓뭹?낅땲?? 紐⑤뱺 ?쒗쁽? 臾명솕??留λ씫?먯꽌 ?댄빐?섏뼱???섎ŉ, ?몄쐞?곸씤 蹂???놁씠 ??룆?⑸땲??]\n\n`;
    const numLines = processedPrompt.split('\n').filter(l => l.trim()).length;

    if (instructions.length > 0) {
      if (isNativeAudio) {
        const voiceInfo = speechConfig.voiceConfig?.prebuiltVoiceConfig.voiceName || 'Professional Actor';
        const isFemale = ['Zephyr', 'Kore', 'Leda', 'Aoede', 'Callirrhoe', 'Autonoe', 'Despina', 'Erinome', 'Laomedeia', 'Gacrux', 'Pulcherrima', 'Vindemiatrix', 'Sulafat'].includes(voiceInfo);

        finalPrompt = `${contextPrefix}[珥덉젙諛 TTS 紐⑤뱶 - ?덈? 洹쒖튃]
- ?뱀떊? 李쎌쓽?곸씤 李쎌옉?먭? ?꾨땲?? ?낅젰???띿뒪?몃? ?덈뒗 洹몃?濡??뚮━?댁뼱 ?쎈뒗 **?꾨Ц TTS ?붿쭊**?낅땲??
- **留ㅼ슦 以묒슂**: ?꾨옒 ?蹂몄쓽 紐⑤뱺 湲?먮? ??湲?먮룄 鍮좎쭚?놁씠, 異붽? ?놁씠, 蹂???놁씠 **?묎컳??* ?쎌쑝?몄슂.
- ?뱁엳 "?먮컯?먮컯", "???? ???앸왂?섍린 ?ъ슫 遺?ъ뼱? 諛섎났?섎뒗 ?⑥뼱?ㅼ쓣 ?덈? 嫄대꼫?곗? 留먭퀬 ?뺥솗??諛쒖쓬?섏꽭??
- **留ㅼ슦 以묒슂**: ?꾨옒 ?蹂몄쓽 紐⑤뱺 湲?먮? ??湲?먮룄 鍮좎쭚?놁씠, 異붽? ?놁씠, 蹂???놁씠 **?묎컳??* ?쎌쑝?몄슂.
- ?뱁엳 "?먮컯?먮컯", "???? ???앸왂?섍린 ?ъ슫 遺?ъ뼱? 諛섎났?섎뒗 ?⑥뼱?ㅼ쓣 ?덈? 嫄대꼫?곗? 留먭퀬 ?뺥솗??諛쒖쓬?섏꽭??
- **?곌린 媛?대뱶 (Director's Notes)**: ${stylePrompt || "?꾪넻 ?쇰떞??遺꾩쐞湲곕? ?대젮 李⑤텇?섍퀬 ?덇꺽 ?덇쾶 ??룆?섏꽭??"}
- **湲곕낯 吏덇컧**: ?ъ빞 ?쇰뵒?ㅼ쿂???곕쑜?섍퀬 遺?쒕윭??諛쒖꽦???좎??섎ŉ, ?뚯같???? ???????뺤젣?섏뿬 ?ｊ린 ?명븳 ?뚮━瑜??댁꽭??
- AI濡쒖꽌???먯븘瑜?踰꾨━怨??ㅼ쭅 ??룆?먮쭔 吏묒쨷?섏꽭?? 以묎컙???덈? 硫덉텛嫄곕굹 ?앸왂?섏? 留덉꽭??

# AUDIO PROFILE: ${voiceInfo} - The Professional ${isFemale ? 'Female' : 'Male'} Narrator
## THE SCENE: A high-end recording studio.
### DIRECTOR'S NOTES
- **Accuracy (CRITICAL)**: ?꾨옒 ?蹂?珥?**${numLines}以?*??泥섏쓬遺??留덉?留됯퉴吏 **?????⑥뼱??鍮좎쭚?놁씠 ?꾨?** ??룆?섏꽭??
- **Pacing**: ${speed !== 1.0 ? `Delivered at a ${speed}x pace.` : 'Natural and conversational.'}

[Text to Read - 珥?${numLines}以?
${processedPrompt}

[?蹂???- ?ш린源뚯? 紐⑤뱺 湲?먮? ???쎌뼱???⑸땲??`;
      } else {
        finalPrompt = `${contextPrefix}[Precision TTS Mode]
Read the following text EXACTLY as written. DO NOT skip any words, sentences, or punctuation. DO NOT add any filler words or change the wording.

[Strict Instructions]
1. Read the text EXACTLY as written in the [Text to Read] section.
2. DO NOT skip any words like "?먮컯?먮컯" or "????. Every word is essential.
3. **Voice Consistency & Quality**: Maintain a strictly consistent voice.
${instructions.map((inst, idx) => `${idx + 4}. ${inst}`).join('\n')}

[Text to Read]
${processedPrompt}`;
      }
    } else {
      // Even if no instructions, add context and strict precision rules
      finalPrompt = `${contextPrefix}[珥덉젙諛 TTS 紐⑤뱶: ?꾨옒 ?蹂?${numLines}以꾩쓽 紐⑤뱺 ?⑥뼱瑜???湲?먮룄 鍮좎쭚?놁씠 ?뺥솗??洹몃?濡???룆?섏꽭?? ?덈?濡??⑥뼱瑜??앸왂?섍굅??諛붽씀吏 留덉꽭??]\n\n${processedPrompt}\n\n[?蹂???- ?ш린源뚯? 紐⑤몢 ?쎌뼱二쇱꽭??`;
    }

    if (!generalAI || !liveAI) {
      throw new Error("API ?ㅺ? ?ㅼ젙?섏? ?딆븯?듬땲??");
    }

    // --- CASE 1: Multimodal Live API (WebSocket) for Native Audio Dialog ---
    if (isNativeAudio) {
      console.log(`[Gemini Live API] Delegating to Multi-Turn generator...`);
      const lines = processedPrompt.split('\n').filter(l => l.trim().length > 0);
      const voiceInfo = speechConfig.voiceConfig?.prebuiltVoiceConfig.voiceName || 'Kore';

      const result = await generateAudioWithLiveAPIMultiTurn(
        lines,
        voiceInfo,
        stylePrompt || "Professional Korean Voice Narrator",
        speed,
        500, // Default 500ms silence
        signal
      );

      return uint8ArrayToBase64(new Uint8Array(result.audioBuffer));
    }

    // --- CASE 2: Standard REST API (generateContent) for Flash/Pro TTS ---
    // Use unified SDK style: generalAI.models.generateContent
    console.log(`[Gemini API Request] Model: ${modelName}, Prompt Length: ${finalPrompt.length}`);

    const result = await (generalAI as any).models.generateContent({
      model: modelName,
      contents: [{ role: 'user', parts: [{ text: finalPrompt }] }],
      config: {
        responseModalities: ['AUDIO'],
        speechConfig: isNativeAudio ? undefined : speechConfig,
        generationConfig: {
          temperature: 0.2, // Balance between voice consistency and emotional richness
        }
      },
      safetySettings: [
        { category: 'HARM_CATEGORY_HARASSMENT' as any, threshold: 'BLOCK_NONE' as any },
        { category: 'HARM_CATEGORY_HATE_SPEECH' as any, threshold: 'BLOCK_NONE' as any },
        { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT' as any, threshold: 'BLOCK_NONE' as any },
        { category: 'HARM_CATEGORY_DANGEROUS_CONTENT' as any, threshold: 'BLOCK_NONE' as any },
        { category: 'HARM_CATEGORY_CIVIC_INTEGRITY' as any, threshold: 'BLOCK_NONE' as any },
      ]
    });

    const response = result; // result inside (generalAI as any).models.generateContent is the parsed response directly

    console.log("[Gemini API Full Response]", JSON.stringify(response, null, 2));

    const candidate = response.candidates?.[0];

    // Check finishReason
    if (candidate?.finishReason && candidate.finishReason !== 'STOP' && candidate.finishReason !== 'MAX_TOKENS') {
      console.warn(`[Gemini TTS] Unusual finishReason: ${candidate.finishReason}. Audio might be truncated.`);
    }

    const audioPart = candidate?.content?.parts.find((part: any) => part.inlineData);
    const data = audioPart?.inlineData?.data;

    if (!data) {
      console.error("API response missing audio. Full candidate:", candidate);

      if (candidate?.finishReason === 'SAFETY') {
        throw new Error(`?덉쟾 ?꾪꽣???섑빐 李⑤떒?섏뿀?듬땲?? (FinishReason: SAFETY). 臾몄젣媛 ???띿뒪???쇰?: "${prompt.substring(0, 100)}..."`);
      }
      if (candidate?.finishReason === 'PROHIBITED_CONTENT') {
        const blockPreview = prompt.length > 500 ? prompt.substring(0, 500) + "..." : prompt;
        console.error("李⑤떒???띿뒪??泥?겕 ?꾩껜:", prompt);
        throw new Error(`援ш? ?뺤콉???섑빐 李⑤떒??肄섑뀗痢좎엯?덈떎. (FinishReason: PROHIBITED_CONTENT). ?대떦 泥?겕???ы븿???뱀젙 ?⑥뼱???쒗쁽???섏젙??蹂댁꽭?? (李⑤떒??援ш컙 ?쒖옉: "${blockPreview}")`);
      }
      if (candidate?.finishReason === 'RECITATION') {
        throw new Error(`??묎텒???덈뒗 ?띿뒪?몃줈 媛먯??섏뼱 李⑤떒?섏뿀?듬땲?? (FinishReason: RECITATION).`);
      }
      if (candidate?.finishReason === 'OTHER') {
        throw new Error(`?????녿뒗 ?댁쑀濡?紐⑤뜽??李⑤떒?섏뿀?듬땲?? (FinishReason: OTHER).`);
      }

      const textPart = candidate?.content?.parts?.[0]?.text;
      if (textPart) {
        console.warn("Model responded with text instead of audio:", textPart);
        throw new Error(`AI媛 ?ㅻ뵒??????띿뒪?몃줈 ?묐떟?덉뒿?덈떎: "${textPart.substring(0, 150)}..."`);
      }

      throw new Error(`?ㅻ뵒???곗씠???꾨씫 (FinishReason: ${candidate?.finishReason || 'UNKNOWN'}). API ?묐떟 援ъ“媛 ?됱냼? ?ㅻ쫭?덈떎. 肄섏넄 濡쒓렇瑜??뺤씤?댁＜?몄슂.`);
    }

    return data;
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      throw error;
    }
    console.error("Error generating audio with Gemini API:", error);
    throw handleApiError(error);
  }
}

export const generateSingleSpeakerAudio = (
  prompt: string,
  voiceName: string,
  modelName: string,
  speed: number = 1.0,
  stylePrompt?: string,
  signal?: AbortSignal
): Promise<string> => {
  const speechConfig: SpeechConfig = {
    voiceConfig: {
      prebuiltVoiceConfig: {
        voiceName: voiceName,
      },
    },
    languageCode: 'ko-KR',
  };
  return _generateAudio(prompt, modelName, speechConfig, speed, stylePrompt, signal);
};

export const previewVoice = (voiceName: string): Promise<string> => {
  const sampleText = `?덈뀞?섏꽭?? ?닿쾬? ??紐⑹냼由ъ엯?덈떎. ??紐⑹냼由щ줈 硫뗭쭊 ?ㅻ뵒??肄섑뀗痢좊? 留뚮뱾 ???덉뒿?덈떎.`;
  // Use default Flash model for previews to save cost/latency
  const defaultModel = "gemini-2.5-flash-preview-tts";
  return generateSingleSpeakerAudio(sampleText, voiceName, defaultModel, 1.0);
};

export const transcribeAudioWithSrt = async (
  base64Wav: string,
  splitCharCount: number,
  signal?: AbortSignal,
  referenceText?: string,
  speed: number = 1.0
): Promise<string> => {
  if (!generalAI) {
    throw new Error("API ?ㅺ? ?ㅼ젙?섏? ?딆븯?듬땲?? ?곗륫 ?곷떒 ?ㅼ젙 ?꾩씠肄섏쓣 ?뚮윭 API ?ㅻ? ?낅젰?댁＜?몄슂.");
  }

  try {
    const audioPart = {
      inlineData: {
        mimeType: 'audio/wav',
        data: base64Wav,
      },
    };

    const processedReference = referenceText
      ?.replace(/\.\.\./g, ', ')
      .replace(/??g, ', ');
    const numLinesSrt = processedReference?.split('\n').filter(l => l.trim()).length || 'N';

    let promptText = `??븷: 珥덉젙諛 ?먮쭑 ?쒖옉??(Ultra-Precise Subtitler)
紐⑺몴: ?쒓났??[?蹂??뺣낫]? [?ㅻ뵒??瑜?1:1濡?留ㅼ묶?섏뿬 ?꾨꼍??SRT ?앹꽦 (?덈? 以꾩쓣 ?⑹튂嫄곕굹 ?섎늻吏 留?寃?

[吏移?
1. ?낅젰 ?蹂몄쓽 珥?以??섎뒗 ?뺥솗??**${numLinesSrt}以?*?낅땲??
2. **留ㅼ슦 以묒슂**: 媛?以꾩? ?쇳몴(,)??留덉묠??.)媛 ?덈뜑?쇰룄 ?덈? ??媛??댁긽??SRT ?뷀듃由щ줈 ?섎늻吏 留덉떗?쒖삤.
3. ?낅젰 ?곗씠?곗쓽 1踰?以꾩? 臾댁“嫄?SRT??1踰? 2踰?以꾩? SRT??2踰덉씠 ?섏뼱???⑸땲??
4. ?? "???? ???? ?먮컯?먮컯 ???대젮媛?寃껋?," ????以꾩씠?쇰㈃, 諛섎뱶?????섎굹????꾩뒪?ы봽 援ш컙?쇰줈 ?앹꽦?섏떗?쒖삤. ?덈?濡??쇳몴?먯꽌 ?먮Ⅴ吏 留덉꽭??
5. ?ㅻ뵒???댁슜???蹂멸낵 100% ?쇱튂?댁빞 ?섎ŉ, ??꾩뒪?ы봽???ㅼ젣 諛쒖쓬 援ш컙???곕씪???⑸땲??

**諛곌꼍 ?뺣낫**:
- ???ㅻ뵒?ㅻ뒗 ??**${speed}諛곗냽**?쇰줈 ?앹꽦?섏뿀?듬땲?? ?쇰컲 ?뺣같?띾낫???먮┫ ???덉쑝????꾩뒪?ы봽瑜??좎쨷?섍쾶 ?ㅼ젙?섏꽭??
- [而⑦뀓?ㅽ듃]: ?닿쾬? ?쒓뎅 ?꾪넻 ?쇰떞/誘쇰떞 ??룆?낅땲?? ??궗???덉닠 ?묓뭹?쇰줈 ??고븯??떆??
- ???띿뒪?몃뒗 珥?**${referenceText?.split('\n').filter(l => l.trim()).length || '?????놁쓬'}** 以꾨줈 援ъ꽦???ш레 ?蹂몄엯?덈떎.

**理쒖슦??洹쒖튃 (?덈? 以??:**
1. **?띿뒪??蹂댁〈 (TEXT PRESERVATION)**: ?꾨옒 [李몄“ ?蹂????쒓났???띿뒪?몃? **????湲?? ???섎굹??湲고샇???섏젙?섏? 留덉꽭??** AI媛 ?꾩궗?섏? 留먭퀬, ?쒓났???띿뒪?몃? 洹몃?濡?SRT???ъ슜?섏꽭??
2. **1:1 以??쒓컙 留ㅼ묶 (LINE-TO-TIME MAPPING)**: [李몄“ ?蹂???媛?以?Line)???ㅻ뵒?ㅼ뿉???섑??섎뒗 **?쒖옉 ?쒓컙怨?醫낅즺 ?쒓컙留??뺥솗??李얠븘?댁뼱 SRT 釉붾줉?쇰줈 留뚮뱶?몄슂.**
3. **以???遺덉씪移?湲덉?**: 理쒖쥌 SRT 釉붾줉??媛쒖닔??[李몄“ ?蹂???以??섏? 臾댁“嫄??쇱튂?댁빞 ?⑸땲?? ?⑹튂湲? 履쇨컻湲? 嫄대꼫?곌린 紐⑤몢 ?꾧꺽??湲덉??⑸땲??
4. **?щ㎎ 以??*: ?쒖? SubRip(.srt) ?뺤떇???곕Ⅴ?? ?댁슜? 諛섎뱶???쒓났???蹂몄쓣 ?좎뵪 ?섎굹 ?由ъ? ?딄퀬 ?ъ슜?섏꽭??
5. **??꾩뒪?ы봽 ?뺣???*: 0.9諛곗냽 ?ㅻ뵒???뚰삎??留욎떠 臾몄옣???쒖옉?섍퀬 ?앸굹??吏?먯쓣 諛由ъ큹 ?⑥쐞濡??뺥솗???≪쑝?몄슂.

**[李몄“ ?蹂?:**
${referenceText}

**異쒕젰**: 肄붾뱶 釉붾줉(\`\`\`srt) ?덉뿉 SRT ?댁슜留?異쒕젰?섏꽭??`;

    if (referenceText) {
      promptText += `

**[紐⑤뱶: 媛뺤젣 ?뺣젹 (Forced Alignment)]**
**諛곌꼍**: ???ㅻ뵒?ㅻ뒗 李쎌옉 ?ш레 ?쒕씪留덉쓽 ?쇰??낅땲??
?쒓났??**李몄“ ?ㅽ겕由쏀듃**媛 ???ㅻ뵒?ㅼ쓽 ?뺥솗???蹂몄엯?덈떎. ???ㅽ겕由쏀듃??紐⑤뱺 以꾩쓣 ?ㅻ뵒???뚰삎怨??쇱튂?쒖폒 SRT瑜??앹꽦?섏꽭??

**??以묒슂 ?먮쭑 洹쒖튃 (?꾨룆):**
1. **?꾨씫 湲덉?:** 李몄“ ?ㅽ겕由쏀듃??紐⑤뱺 臾몄옣???덉쇅 ?놁씠 ?먮쭑?쇰줈 留뚮뱶?몄슂.
2. **泥?臾몄옣 (0珥??쒖옉):** 泥??먮쭑? 臾댁“嫄?00:00:00,000?먯꽌 ?쒖옉?댁빞 ?⑸땲??
3. **寃뱀묠 諛⑹?:** (N)踰덉㎏ ?먮쭑 醫낅즺 ?쒓컙 < (N+1)踰덉㎏ ?먮쭑 ?쒖옉 ?쒓컙???섎룄濡??섏꽭??
4. **媛?낆꽦 遺꾪븷:** ??以꾩씠 ?덈Т 湲몃㈃(${splitCharCount}???댁긽) ?섎? ?⑥쐞濡??먯뿰?ㅻ읇寃???以꾨줈 ?섎늻?몄슂.

**[李몄“ ?ㅽ겕由쏀듃]:**
${referenceText}`;
    } else {
      promptText += `

**[紐⑤뱶: ?쇰컲 ?꾩궗 (Transcription)]**
1. ?ㅻ뵒?ㅻ? ?ｊ퀬 ?댁슜???뺥솗?섍쾶 ?쒓뎅?대줈 諛쏆븘?곗꽭??
2. 臾몃㎘??留욊쾶 ?먯뿰?ㅻ읇寃?以꾩쓣 ?섎늻???먮쭑???앹꽦?섏꽭??
3. ?먮쭑 ??以꾩? 理쒕? ${splitCharCount}?먮? ?섏? ?딅룄濡??섏꽭??
`;
    }

    promptText += `

**異쒕젰 ?덉떆:**
1
00:00:00,000 --> 00:00:02,150
?덈뀞?섏꽭?? AI 蹂댁씠???ㅽ뒠?붿삤?낅땲??

2
00:00:02,250 --> 00:00:05,100
?띿뒪?몃? ?낅젰?섎㈃ 紐⑹냼由щ줈 蹂?섑빐?쒕┰?덈떎.
`;

    const textPart = { text: promptText };

    // ???좊쾭??臾몃쾿 + gemini-2.5-flash
    const result = await (generalAI as any).models.generateContent({
      model: 'gemini-2.5-flash',
      contents: [{
        role: 'user',
        parts: [audioPart, textPart]
      }],
    });

    // ???묐떟 ?묎렐
    let srtText = result.candidates?.[0]?.content?.parts?.[0]?.text?.trim() ?? '';

    const match = srtText.match(/```(?:srt)?\s*([\s\S]*?)```/);
    if (match && match[1]) {
      srtText = match[1].trim();
    }

    return srtText;
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      throw error;
    }
    console.error("Error transcribing audio with Gemini API:", error);
    throw handleApiError(error);
  }
};

/**
 * Live API ?⑥씪 ?몄뀡 硫?고꽩 諛⑹떇?쇰줈 ?ㅻ뵒???앹꽦
 * - ?몄뀡 1媛??좎?
 * - 以꾨퀎濡??낅┰ ???붿껌
 * - 媛?以??꾨즺 ???ㅼ쓬 以?吏꾪뻾
 */
export async function generateAudioWithLiveAPIMultiTurn(
  lines: string[],
  voiceName: string,
  stylePrompt: string,
  speed: number = 1.0,
  silenceBetweenLinesMs: number = 500,
  signal?: AbortSignal
): Promise<{
  audioBuffer: ArrayBuffer;
  lineTimings: { start: number; end: number }[];
  paragraphs: string[];
}> {

  if (!liveAI) {
    throw new Error("API ?ㅺ? ?ㅼ젙?섏? ?딆븯?듬땲??");
  }

  const audioResults: ArrayBuffer[] = [];
  const lineTimings: { start: number; end: number }[] = [];
  let currentLineAudio: ArrayBuffer[] = [];
  let turnCompleteResolve: (() => void) | null = null;
  let sessionError: Error | null = null;
  let chunkCounter = 0; // ?몄뀡 ?꾩껜 泥?겕 移댁슫??
  // ?좏슚??以꾨쭔 ?꾪꽣留?  const validLines = lines.map(l => l.trim()).filter(l => l.length > 0);

  if (lines.length === 0) {
    throw new Error("?앹꽦???띿뒪?멸? ?놁뒿?덈떎.");
  }

  console.log(`[Gemini Live API] Starting Precision Paragraph-Based Multi-Turn session`);

  // 鍮?以꾩쓣 湲곗??쇰줈 臾몃떒(Batch) ?섎늻湲?  const paragraphs: string[] = [];
  let currentGroup: string[] = [];

  for (const line of lines) {
    if (line.trim().length === 0) {
      if (currentGroup.length > 0) {
        paragraphs.push(currentGroup.join('\n'));
        currentGroup = [];
      }
    } else {
      currentGroup.push(line);
    }
  }
  if (currentGroup.length > 0) {
    paragraphs.push(currentGroup.join('\n'));
  }

  if (paragraphs.length === 0) {
    throw new Error("泥섎━?????덈뒗 ?띿뒪???댁슜???놁뒿?덈떎.");
  }

  console.log(`[Gemini Live API] Processing ${paragraphs.length} paragraphs`);

  return new Promise(async (resolve, reject) => {
    try {
      const liveModel = 'gemini-2.5-flash-native-audio-preview-12-2025';
      const session = await (liveAI as any).live.connect({
        model: liveModel,
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: {
              prebuiltVoiceConfig: {
                voiceName,
              }
            }
          },
          systemInstruction: {
            parts: [{
              text: `[System Instruction]: You are a professional Korean voice actor. 
[Voice Persona]: Warm, calm, and steady late-night radio DJ. Use de-essed, smooth vocal texture.
[Director's Notes]: ${stylePrompt}
[Strict Rules]: 
1. Read the provided text EXACTLY as written. 
2. DO NOT skip any words, symbols, or sentences. 
3. Output ONLY the spoken audio. 
4. DO NOT summarize or interpret.`
            }]
          },
          safetySettings: [
            { category: 'HARM_CATEGORY_HARASSMENT' as any, threshold: 'BLOCK_NONE' as any },
            { category: 'HARM_CATEGORY_HATE_SPEECH' as any, threshold: 'BLOCK_NONE' as any },
            { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT' as any, threshold: 'BLOCK_NONE' as any },
            { category: 'HARM_CATEGORY_DANGEROUS_CONTENT' as any, threshold: 'BLOCK_NONE' as any },
            { category: 'HARM_CATEGORY_CIVIC_INTEGRITY' as any, threshold: 'BLOCK_NONE' as any },
          ],
        },
        callbacks: {
          onopen: () => {
            console.log('[Gemini Live API] WebSocket opened.');
          },
          onmessage: async (response: any) => {
            const isTurnComplete = !!response.serverContent?.turnComplete;

            // ?ㅻ뵒??泥?겕 ?섏쭛
            if (response.serverContent?.modelTurn?.parts) {
              for (const part of response.serverContent.modelTurn.parts) {
                if (part.inlineData?.data) {
                  const chunk = base64ToArrayBuffer(part.inlineData.data);
                  // ?덉젙?깆쓣 ?꾪빐 理쒖냼?쒖쓽 濡쒓렇 異쒕젰 ?좎? (濡쒓렇 異쒕젰 ??諛쒖깮?섎뒗 誘몄꽭 吏?곗씠 ?섏쭛 ?덉젙?붿뿉 ?꾩?)
                  console.log(`[Gemini Live API] Chunk received: ${chunk.byteLength} bytes`);
                  currentLineAudio.push(chunk);
                }
              }
            }

            // ???꾨즺 媛먯? (800ms ?湲고븯??留덉?留?泥?겕 ?섏떊 蹂댁옣)
            if (isTurnComplete) {
              console.log(`[Gemini Live API] turnComplete received. Starting 800ms protection delay...`);
              const resolveRef = turnCompleteResolve;
              turnCompleteResolve = null;
              if (resolveRef) {
                setTimeout(() => {
                  console.log(`[Gemini Live API] 800ms delay finished. Resolving turn.`);
                  resolveRef();
                }, 800);
              }
            }

            // ?명꽣?쏀듃 媛먯?
            if (response.serverContent?.interrupted) {
              console.warn(`[Gemini Live API] Server sent "interrupted" signal. Waiting for turnComplete anyway...`);
            }
          },
          onerror: (e: any) => {
            console.error('[Gemini Live API] Error:', e);
            sessionError = new Error(e.message || 'Live API ?ㅻ쪟');
            if (turnCompleteResolve) {
              turnCompleteResolve();
            }
          },
          onclose: (e: any) => {
            console.log(`[Gemini Live API] WebSocket closed. (Code ${e?.code || 'unknown'})`);
          }
        }
      });

      // ?몄뀡 ?곌껐 ?꾨즺 ??硫?고꽩 泥섎━ ?쒖옉
      console.log('[Gemini Live API] Connected. Starting multi-turn loop...');

      let cumulativeTimeMs = 0;

      for (let i = 0; i < paragraphs.length; i++) {
        // 以묐떒 ?좏샇 ?뺤씤
        if (signal?.aborted) {
          session.close();
          throw new Error('?ъ슜?먯뿉 ?섑빐 以묐떒?섏뿀?듬땲??');
        }

        const batchText = paragraphs[i];

        // 留먯쨪?꾪몴 移섑솚 ????먮낯 ?띿뒪???좎? (?ъ슜?먮떂 愿李?諛섏쁺)
        const processedBatch = batchText;

        console.log(`[Gemini Live API] Requesting Paragraph ${i + 1}/${paragraphs.length}: "${processedBatch.substring(0, 30).replace(/\n/g, ' ')}..."`);

        // ?꾩옱 以??ㅻ뵒??珥덇린??諛?泥?겕 移댁슫??由ъ뀑
        currentLineAudio = [];
        chunkCounter = 0;

        // ???꾨즺 ?湲?Promise ?앹꽦
        const turnCompletePromise = new Promise<void>((res) => {
          turnCompleteResolve = res;
        });

        // Send the batch with a clear instruction
        const linePrompt = `Please read this text exactly: "${processedBatch}"`;

        await session.sendClientContent({
          turns: [{ role: 'user', parts: [{ text: linePrompt }] }],
          turnComplete: true
        });

        // ???꾨즺 ?湲?        await turnCompletePromise;

        // ?먮윭 泥댄겕
        if (sessionError) {
          session.close();
          throw sessionError;
        }

        // 寃곌낵 ???        const lineAudio = mergeArrayBuffers(currentLineAudio);
        audioResults.push(lineAudio);

        // ??대컢 怨꾩궛 (24kHz, 16bit 湲곗?)
        // Note: Gemini standard sampling rate for Native Audio is often 24kHz or 16kHz. 
        // We'll use 24kHz as per user instructions.
        const lineDurationMs = (lineAudio.byteLength / 2 / 24000) * 1000;
        lineTimings.push({
          start: cumulativeTimeMs,
          end: cumulativeTimeMs + lineDurationMs
        });
        cumulativeTimeMs += lineDurationMs + silenceBetweenLinesMs;

        console.log(`[Gemini Live API] Line ${i + 1} completed. ${lineAudio.byteLength} bytes, ${lineDurationMs.toFixed(0)}ms`);
      }

      // ?몄뀡 醫낅즺
      session.close();

      // 臾댁쓬 ?쎌엯?섏뿬 理쒖쥌 蹂묓빀
      console.log(`[Gemini Live API] Merging ${audioResults.length} audio segments with ${silenceBetweenLinesMs}ms silence...`);
      const finalAudio = mergeAudioWithSilence(audioResults, silenceBetweenLinesMs);

      console.log(`[Gemini Live API] Complete! Total: ${finalAudio.byteLength} bytes`);

      resolve({
        audioBuffer: finalAudio,
        lineTimings: lineTimings,
        paragraphs: paragraphs
      });

    } catch (error) {
      reject(error);
    }
  });
}
